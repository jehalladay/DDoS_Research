{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ba9add6",
   "metadata": {},
   "source": [
    "# CIC-Darknet2020 Dataset Cleaning\n",
    "\n",
    "Here we load data from the [CIC-Darknet2020](https://www.unb.ca/cic/datasets/darknet2020.html) dataset remove any invalid values from the dataset and save the cleaned dataset to a new file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db93b9",
   "metadata": {},
   "source": [
    "First we import all relevant libraries, set a random seed, and print python and library versions for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b16de8a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Last Execution: 2022-02-16 20:59:06.427550\n",
      "    python:\t3.7.10\n",
      "\n",
      "    \tmatplotlib:\t3.3.4\n",
      "    \tnumpy:\t\t1.20.3\n",
      "    \tpandas:\t\t1.2.5\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import datetime, os, platform, pprint, sys\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "seed: int = 14\n",
    "\n",
    "# set up pretty printer for easier data evaluation\n",
    "pretty = pprint.PrettyPrinter(indent=4, width=30).pprint\n",
    "\n",
    "# set up pandas display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\n",
    "    f'''\n",
    "    Last Execution: {datetime.datetime.now()}\n",
    "    python:\\t{platform.python_version()}\n",
    "\n",
    "    \\tmatplotlib:\\t{mpl.__version__}\n",
    "    \\tnumpy:\\t\\t{np.__version__}\n",
    "    \\tpandas:\\t\\t{pd.__version__}\n",
    "    '''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5f4718",
   "metadata": {},
   "source": [
    "Next we prepare some helper functions to help process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96f30df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_path(directory: str):\n",
    "    '''\n",
    "        Closure that will return a function. \n",
    "        Function will return the filepath to the directory given to the closure\n",
    "    '''\n",
    "\n",
    "    def func(file: str) -> str:\n",
    "        return os.path.join(directory, file)\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "\n",
    "def load_data(filePath):\n",
    "    '''\n",
    "        Loads the Dataset from the given filepath and caches it for quick access in the future\n",
    "        Function will only work when filepath is a .csv file\n",
    "    '''\n",
    "\n",
    "    # slice off the ./CSV/ from the filePath\n",
    "    if filePath[0] == '.' and filePath[1] == '/':\n",
    "        filePathClean: str = filePath[11::]\n",
    "        pickleDump: str = f'./cache/{filePathClean}.pickle'\n",
    "    else:\n",
    "        pickleDump: str = f'./cache/{filePath}.pickle'\n",
    "    \n",
    "    print(f'Loading Dataset: {filePath}')\n",
    "    print(f'\\tTo Dataset Cache: {pickleDump}\\n')\n",
    "\n",
    "\n",
    "    # check if data already exists within cache\n",
    "    if os.path.exists(pickleDump):\n",
    "        df = pd.read_pickle(pickleDump)\n",
    "        df['Label1'] = df['Label1'].str.lower()\n",
    "        df.Label1.unique()    \n",
    "\n",
    "    # if not, load data and clean it before caching it\n",
    "    else:\n",
    "        df = pd.read_csv(filePath, low_memory=True)\n",
    "        df['Label1'] = df['Label1'].str.lower()\n",
    "        df.Label1.unique()    \n",
    "        df.to_pickle(pickleDump)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def features_with_bad_values(df: pd.DataFrame, datasetName: str) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will scan the dataframe for features with Inf, NaN, or Zero values.\n",
    "        Returns a new dataframe describing the distribution of these values in the original dataframe\n",
    "    '''\n",
    "\n",
    "    # Inf and NaN values can take different forms so we screen for every one of them\n",
    "    invalid_values: list = [ np.inf, np.nan, 'Infinity', 'inf', 'NaN', 'nan', 0 ]\n",
    "    infs          : list = [ np.inf, 'Infinity', 'inf' ]\n",
    "    NaNs          : list = [ np.nan, 'NaN', 'nan' ]\n",
    "\n",
    "    # We will collect stats on the dataset, specifically how many instances of Infs, NaNs, and 0s are present.\n",
    "    # using a dictionary that will be converted into a (3, 2+88) dataframe\n",
    "    stats: dict = {\n",
    "        'Dataset':[ datasetName, datasetName, datasetName ],\n",
    "        'Value'  :['Inf', 'NaN', 'Zero']\n",
    "    }\n",
    "\n",
    "    i = 0\n",
    "    for col in df.columns:\n",
    "        \n",
    "        i += 1\n",
    "        feature = np.zeros(3)\n",
    "        \n",
    "        for value in invalid_values:\n",
    "            if value in infs:\n",
    "                j = 0\n",
    "            elif value in NaNs:\n",
    "                j = 1\n",
    "            else:\n",
    "                j = 2\n",
    "            indexNames = df[df[col] == value].index\n",
    "            if not indexNames.empty:\n",
    "                feature[j] += len(indexNames)\n",
    "                \n",
    "        stats[col] = feature\n",
    "\n",
    "    return pd.DataFrame(stats)\n",
    "\n",
    "\n",
    "\n",
    "def clean_data(df: pd.DataFrame, prune: list) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will take a dataframe and remove the columns that match a value in prune \n",
    "        Inf values will also be removed from Flow Bytes/s and Flow Packets/s\n",
    "        once appropriate rows and columns have been removed, we will return\n",
    "        the dataframe with the appropriate values\n",
    "    '''\n",
    "\n",
    "    # remove the features in the prune list    \n",
    "    for col in prune:\n",
    "        if col in df.columns:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    \n",
    "    # drop missing values/NaN etc.\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    \n",
    "    # Search through dataframe for any Infinite or NaN values in various forms that were not picked up previously\n",
    "    invalid_values: list = [\n",
    "        np.inf, np.nan, 'Infinity', 'inf', 'NaN', 'nan'\n",
    "    ]\n",
    "    \n",
    "    for col in df.columns:\n",
    "        for value in invalid_values:\n",
    "            indexNames = df[df[col] == value].index\n",
    "            if not indexNames.empty:\n",
    "                print(f'deleting {len(indexNames)} rows with Infinity in column {col}')\n",
    "                df.drop(indexNames, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebcd0e5",
   "metadata": {},
   "source": [
    "Before we do any processing on the data, we need to list out all their filepaths. If trying to reproduce the process carried out here, place files in the same location relative to the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18ba1f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used to scale to processing numerous datasets, even though we currently are only looking at one now\n",
    "data_path_1: str = './original/'   \n",
    "data_set_1: list = [\n",
    "    'Darknet.csv',\n",
    "]\n",
    "\n",
    "data_set: list  = data_set_1\n",
    "file_path_1      = get_file_path(data_path_1)\n",
    "file_set: list   = list(map(file_path_1, data_set_1))\n",
    "current_job: int = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43958466",
   "metadata": {},
   "source": [
    "Some more helper functions that process the data using the file and dataset information above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87095782",
   "metadata": {},
   "outputs": [],
   "source": [
    "def examine_dataset(job_id: int) -> dict({'File': str, 'Dataset': pd.DataFrame, 'Feature_stats': pd.DataFrame, 'Data_composition': pd.DataFrame}):\n",
    "    '''\n",
    "        Function will return a dictionary containing dataframe of the job_id passed in as well as that dataframe's\n",
    "        feature stats, data composition, and file name.\n",
    "    '''\n",
    "\n",
    "    job_id = job_id - 1  # adjusts for indexing while enumerating jobs from 1\n",
    "    print(f'Dataset {job_id+1}/{len(data_set)}: We now look at {file_set[job_id]}\\n\\n')\n",
    "\n",
    "\n",
    "    # Load the dataset\n",
    "    df: pd.DataFrame = load_data(file_set[job_id])\n",
    "\n",
    "\n",
    "    # print the data composition\n",
    "    print(f'''\n",
    "        File:\\t\\t\\t\\t{file_set[job_id]}  \n",
    "        Job Number:\\t\\t\\t{job_id+1}\n",
    "        Shape:\\t\\t\\t\\t{df.shape}\n",
    "        Samples:\\t\\t\\t{df.shape[0]} \n",
    "        Features:\\t\\t\\t{df.shape[1]}\n",
    "    ''')\n",
    "\n",
    "\n",
    "    # return the dataframe and the feature stats\n",
    "    data_summary: dict =  {\n",
    "        'File':             file_set[job_id],\n",
    "        'Dataset':          df,\n",
    "        'Feature_stats':    features_with_bad_values(df, file_set[job_id]), \n",
    "    }\n",
    "    \n",
    "    return data_summary\n",
    "\n",
    "\n",
    "def package_data_for_inspection(df: pd.DataFrame) -> dict({'File': str, 'Dataset': pd.DataFrame, 'Feature_stats': pd.DataFrame, 'Data_composition': pd.DataFrame}):\n",
    "    '''\n",
    "        Function will return a dictionary containing dataframe passed in as well as that dataframe's feature stats.\n",
    "    '''\n",
    "\n",
    "    # print the data composition\n",
    "    print(f'''\n",
    "        Shape:\\t\\t\\t\\t{df.shape}\n",
    "        Samples:\\t\\t\\t{df.shape[0]} \n",
    "        Features:\\t\\t\\t{df.shape[1]}\n",
    "    ''')\n",
    "    \n",
    "\n",
    "    # return the dataframe and the feature stats\n",
    "    data_summary: dict =  {\n",
    "        'File':             '',\n",
    "        'Dataset':          df,\n",
    "        'Feature_stats':    features_with_bad_values(df, ''), \n",
    "    }\n",
    "    \n",
    "    return data_summary\n",
    "\n",
    "\n",
    "def package_data_for_inspection_with_label(df: pd.DataFrame, label: str) -> dict({'File': str, 'Dataset': pd.DataFrame, 'Feature_stats': pd.DataFrame, 'Data_composition': pd.DataFrame}):\n",
    "    '''\n",
    "        Function will return a dictionary containing dataframe passed in as well as that dataframe's feature stats.\n",
    "    '''\n",
    "\n",
    "    # print the data composition\n",
    "    print(f'''\n",
    "        Shape:\\t\\t\\t\\t{df.shape}\n",
    "        Samples:\\t\\t\\t{df.shape[0]} \n",
    "        Features:\\t\\t\\t{df.shape[1]}\n",
    "    ''')\n",
    "    \n",
    "\n",
    "    # return the dataframe and the feature stats\n",
    "    data_summary: dict =  {\n",
    "        'File':             f'{label}',\n",
    "        'Dataset':          df,\n",
    "        'Feature_stats':    features_with_bad_values(df, f'{label}'), \n",
    "    }\n",
    "    \n",
    "    return data_summary\n",
    "\n",
    "\n",
    "\n",
    "def check_infs(data_summary: dict) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of Inf.\n",
    "    '''\n",
    "\n",
    "    \n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    inf_df = vals[vals['Value'] == 'Inf'].T\n",
    "\n",
    "    return inf_df[inf_df[0] != 0]\n",
    "\n",
    "\n",
    "\n",
    "def check_nans(data_summary: dict) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of NaN.\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    nan_df = vals[vals['Value'] == 'NaN'].T\n",
    "\n",
    "    return nan_df[nan_df[1] != 0]\n",
    "\n",
    "\n",
    "\n",
    "def check_zeros(data_summary: dict) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of 0.\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    zero_df = vals[vals['Value'] == 'Zero'].T\n",
    "\n",
    "    return zero_df[zero_df[2] != 0]\n",
    "\n",
    "\n",
    "\n",
    "def check_zeros_over_threshold(data_summary: dict, threshold: int) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with a value of 0.\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    zero_df = vals[vals['Value'] == 'Zero'].T\n",
    "    zero_df_bottom = zero_df[2:]\n",
    "\n",
    "    return zero_df_bottom[zero_df_bottom[2] > threshold]\n",
    "\n",
    "\n",
    "\n",
    "def check_zeros_over_threshold_percentage(data_summary: dict, threshold: float) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return a dataframe of features with all features with\n",
    "        a frequency of 0 values greater than the threshold\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    size: int = data_summary['Dataset'].shape[0]\n",
    "    zero_df = vals[vals['Value'] == 'Zero'].T\n",
    "    zero_df_bottom = zero_df[2:]\n",
    "\n",
    "    return zero_df_bottom[zero_df_bottom[2] > threshold*size]\n",
    "\n",
    "\n",
    "def remove_infs_and_nans(data_summary: dict) -> pd.DataFrame:\n",
    "    '''\n",
    "        Function will return the dataset with all inf and nan values removed.\n",
    "    '''\n",
    "\n",
    "    df: pd.DataFrame = data_summary['Dataset']\n",
    "    df = clean_data(df, [])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def create_new_prune_candidates(zeros_df: pd.DataFrame) -> list:\n",
    "    '''\n",
    "        Function creates a list of prune candidates from a dataframe of features with a high frequency of 0 values\n",
    "    '''\n",
    "\n",
    "    return list(zeros_df.T.columns)\n",
    "\n",
    "\n",
    "\n",
    "def intersection_of_prune_candidates(pruneCandidates: list, newPruneCandidates: list) -> list:\n",
    "    '''\n",
    "        Function will return a list of features that are in both pruneCandidates and newPruneCandidates\n",
    "    '''\n",
    "\n",
    "    return list(set(pruneCandidates).intersection(newPruneCandidates))\n",
    "\n",
    "\n",
    "\n",
    "def test_infs(data_summary: dict) -> bool:\n",
    "    '''\n",
    "        Function asserts the dataset has no inf values.\n",
    "    '''\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    inf_df = vals[vals['Value'] == 'Inf'].T\n",
    "\n",
    "    assert inf_df[inf_df[0] != 0].shape[0] == 2, 'Dataset has inf values'\n",
    "    \n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "def test_nans(data_summary: dict) -> bool:\n",
    "    '''\n",
    "        Function asserts the dataset has no NaN values\n",
    "    '''\n",
    "\n",
    "    vals: pd.DataFrame = data_summary['Feature_stats']\n",
    "    nan_df = vals[vals['Value'] == 'NaN'].T\n",
    "\n",
    "    assert nan_df[nan_df[1] != 0].shape[0] == 2, 'Dataset has NaN values'\n",
    "\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6f506c",
   "metadata": {},
   "source": [
    "This gives us a set of file locations. Lets look at the set of files that we will be cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26ad9511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We will be cleaning 1 files:\n",
      "['./original/Darknet.csv']\n"
     ]
    }
   ],
   "source": [
    "print(f'We will be cleaning {len(file_set)} files:')\n",
    "pretty(file_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e9f166",
   "metadata": {},
   "source": [
    "## The Original CIC-Darknet2020 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78df4bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 1/1: We now look at ./original/Darknet.csv\n",
      "\n",
      "\n",
      "Loading Dataset: ./original/Darknet.csv\n",
      "\tTo Dataset Cache: ./cache/Darknet.csv.pickle\n",
      "\n",
      "\n",
      "        File:\t\t\t\t./original/Darknet.csv  \n",
      "        Job Number:\t\t\t1\n",
      "        Shape:\t\t\t\t(141530, 85)\n",
      "        Samples:\t\t\t141530 \n",
      "        Features:\t\t\t85\n",
      "    \n",
      "\n",
      "    Labels in the first layer:\n",
      "Label\n",
      "Non-Tor    93356\n",
      "NonVPN     23863\n",
      "Tor         1392\n",
      "VPN        22919\n",
      "dtype: int64\n",
      "\n",
      "    Labels in the second layer:\n",
      " Label1\n",
      "audio-streaming    18064\n",
      "browsing           32808\n",
      "chat               11478\n",
      "email               6145\n",
      "file-transfer      11182\n",
      "p2p                48520\n",
      "video-streaming     9767\n",
      "voip                3566\n",
      "dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_1: dict = examine_dataset(1)\n",
    "print(f\"\"\"\n",
    "    Labels in the first layer:\n",
    "{dataset_1['Dataset'].groupby('Label').size()}\n",
    "\n",
    "    Labels in the second layer:\n",
    " {dataset_1['Dataset'].groupby('Label1').size()}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd0ef16",
   "metadata": {},
   "source": [
    "### Data Inspection\n",
    "\n",
    "Here we just check to see how many inf and nan values are in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de8373e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = check_infs(dataset_1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d38baee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <td>./original/Darknet.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Value</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              1\n",
       "Dataset  ./original/Darknet.csv\n",
       "Value                       NaN"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_nans(dataset_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f160326",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bwd PSH Flags</th>\n",
       "      <td>141530.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd URG Flags</th>\n",
       "      <td>141530.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd URG Flags</th>\n",
       "      <td>141530.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>URG Flag Count</th>\n",
       "      <td>141530.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE Flag Count</th>\n",
       "      <td>141530.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ECE Flag Count</th>\n",
       "      <td>141530.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Bytes/Bulk Avg</th>\n",
       "      <td>141530.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Packet/Bulk Avg</th>\n",
       "      <td>141530.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Bulk Rate Avg</th>\n",
       "      <td>141530.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Bytes/Bulk Avg</th>\n",
       "      <td>141530.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subflow Bwd Packets</th>\n",
       "      <td>141530.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active Mean</th>\n",
       "      <td>141530.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active Std</th>\n",
       "      <td>141530.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active Max</th>\n",
       "      <td>141530.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active Min</th>\n",
       "      <td>141530.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            2\n",
       "Bwd PSH Flags        141530.0\n",
       "Fwd URG Flags        141530.0\n",
       "Bwd URG Flags        141530.0\n",
       "URG Flag Count       141530.0\n",
       "CWE Flag Count       141530.0\n",
       "ECE Flag Count       141530.0\n",
       "Fwd Bytes/Bulk Avg   141530.0\n",
       "Fwd Packet/Bulk Avg  141530.0\n",
       "Fwd Bulk Rate Avg    141530.0\n",
       "Bwd Bytes/Bulk Avg   141530.0\n",
       "Subflow Bwd Packets  141530.0\n",
       "Active Mean          141530.0\n",
       "Active Std           141530.0\n",
       "Active Max           141530.0\n",
       "Active Min           141530.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_zeros_over_threshold(dataset_1, dataset_1['Dataset'].shape[0]-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a3e66e",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Now we remove the infs and nan values and then verify that the dataset no longer contains any infs or nans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67841bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleting 2 rows with Infinity in column Flow Bytes/s\n",
      "\n",
      "        Shape:\t\t\t\t(141481, 85)\n",
      "        Samples:\t\t\t141481 \n",
      "        Features:\t\t\t85\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "dataset_2 = package_data_for_inspection(remove_infs_and_nans(dataset_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e8b19ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Value</th>\n",
       "      <td>Inf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "Dataset     \n",
       "Value    Inf"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_infs(dataset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fdd9494f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dataset</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Value</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           1\n",
       "Dataset     \n",
       "Value    NaN"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_nans(dataset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c33a8fb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bwd PSH Flags</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd URG Flags</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd URG Flags</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>URG Flag Count</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CWE Flag Count</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ECE Flag Count</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Bytes/Bulk Avg</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Packet/Bulk Avg</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fwd Bulk Rate Avg</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bwd Bytes/Bulk Avg</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subflow Bwd Packets</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active Mean</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active Std</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active Max</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active Min</th>\n",
       "      <td>141481.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            2\n",
       "Bwd PSH Flags        141481.0\n",
       "Fwd URG Flags        141481.0\n",
       "Bwd URG Flags        141481.0\n",
       "URG Flag Count       141481.0\n",
       "CWE Flag Count       141481.0\n",
       "ECE Flag Count       141481.0\n",
       "Fwd Bytes/Bulk Avg   141481.0\n",
       "Fwd Packet/Bulk Avg  141481.0\n",
       "Fwd Bulk Rate Avg    141481.0\n",
       "Bwd Bytes/Bulk Avg   141481.0\n",
       "Subflow Bwd Packets  141481.0\n",
       "Active Mean          141481.0\n",
       "Active Std           141481.0\n",
       "Active Max           141481.0\n",
       "Active Min           141481.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_zeros_over_threshold(dataset_2, dataset_2['Dataset'].shape[0]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1878906f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is clean\n"
     ]
    }
   ],
   "source": [
    "if(test_infs(dataset_2) and test_nans(dataset_2)):\n",
    "    print('Dataset is clean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addcf182",
   "metadata": {},
   "source": [
    "### Saving the Cleaned Dataset\n",
    "\n",
    "Now we save the cleaned dataset to a new file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2da0a577",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2['Dataset'].to_csv('./cleaned/Darknet_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdbb77d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c2e742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81898faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Execution: 2022-02-16 20:59:20.803303\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Nothing is complete after this point",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-d2bbcd12aa2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Last Execution: {datetime.datetime.now()}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Nothing is complete after this point'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: Nothing is complete after this point"
     ]
    }
   ],
   "source": [
    "print(f'Last Execution: {datetime.datetime.now()}')\n",
    "assert False, 'Nothing is complete after this point'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51a1e07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
